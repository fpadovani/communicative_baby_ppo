# PPO Training to finetune a Baby and make it more Communicative
To run correctly the code in this repository you need this version of trl==0.8.6 (as specified in the repository). 


## Model
As baseline, we use the model pre-trained by Bastian -> [Baseline_baby](https://huggingface.co/bbunzeck/another-llama)

## PPO Datasets

Our PPO pipeline requires to have some real mother prompts to provide either to our pre-trained Baby model or to our teacher LLM model.
Therefore, I accessed the portion of dialogue data that was generated by Bastian during CHILDES data pre-processing, but not used for pretraining (`childes_dialogue2.txt`) and 
I extracted *MOT: utterances and questions that are not longer than 3 tokens (in order to have meaningful sentences). These mother prompts are stored in the `mother_promps_1.txt` file.

Since prompting the LLM at every training iteration was really computationally heavy, I asked the LLM to generate 10 possible good answers before conducting the training and stored the result in 
`./txt_files/10_answers_2.json`. I also extracted only one answer (not based on any re-ranking, but just selecting the first occurrence) and saved the result in `./txt_files/first_best_options.json`. 

To do so I used the script `answers_generation_vllm_fast.py` with the following prompt to the teacher Llama-3.2-3B:
<pre><code> "You are a young child having a conversation with your mother. "
"When your mother says something, you should answer as a typical kind and natural-sounding child. "
"Do NOT repeat her words. Instead, give a new, relevant answer that shows understanding. "
"Keep it short and child-like."
</code></pre>


## PPO Training

- FIRST ATTEMPTS: Originally we were considering calculating the reward based on the comparison between a child utterance generated by the Baby Model and the 10 Teacher utterances. I tried that but the signal was not very stable given the variability between the 10 teacher utterances. This is way I decided to stick to 1 example from the Teacher (from the `./txt_files/first_best_options.json` file).

- LATER: So all the recent training are based a reward metric calculated between the child utterance and ONLY 1 teacher utterance (simulating a good child response).

### Reward Functions 

1. UNIGRAM BLEU (based on the unigram token overlap between child and teacher utterances)
2. SEMANTIC SIMILARITY (cosine similarity calculated using this model `embedder = SentenceTransformer('all-MiniLM-L6-v2')`


You can fine-tune with PPO the baseline model using this command and providing semsim or bleu as argument:

<pre><code> pyhton ppo_training_blue_semsim --semsim </code></pre>



